\subsection*{Question 2}

\textbf{a)} Assuming independence between the data $\mathcal{D}_i$ collected during the experiment $i$ where $\mathcal{D}_i = \set{y_i(t_j): j = 1,\dots,J = 99}$ with $y_i(t_j) \sim \text{Pois}(\mu(t_j))$, provide an analytic form for the likelihood function $L(\vec{\alpha}|\mathcal{D}_i)$ for experiment $i$.

\begin{center}\rule{6cm}{0.4pt}\end{center}

We know that the number of cancer cells $y_{i,j}$ for the experiment $i$ at any given time $t_j$, $j \in \set{10, 15, 20,...,500}$ follow a Poisson distribution of parameter $\mu$. Its probability mass function is given by,
\begin{equation*}
	p(y_{i,j}) \propto \mu^{y_{i,j}} \cdot e^{-\mu} 
\end{equation*}
where an expression for $\mu$ is given above (\autoref{eq:mu_alpha}) as a function of $\alpha_k (k = 0, 1, 2)$.

Assuming independance, we then have,
\begin{align*}
	L(\vec{\alpha}|\mathcal{D}_i) 
		&= L((\alpha_0, \alpha_1, \alpha_2)|y_i) \\
		&= \prod_{j=1}^{99} p(y_{i, j}) \\
		&= \prod_{j=1}^{99} \left( \mu^{y_{i,j}} \cdot e^{-\mu} \right)
\end{align*}

\textbf{b)} Provide a R function enabling to compute the log-likelihood for given parameter values $\theta_k = \log(\alpha_k)$ and data $D_i$.

\begin{center}\rule{6cm}{0.4pt}\end{center}

We have,
\begin{align*}
	l(\vec{\alpha}|\mathcal{D}_i) 
		&:= \ln(L((\alpha_0, \alpha_1, \alpha_2)|y_i)) \\
		&\propto \sum_{j=1}^{99} \ln\left( \left[ \mu^{y_{i,j}} \cdot e^{-\mu} \right] \right) \\
		&= \sum_{j=1}^{99} \ln(\mu^{y_{i,j}} \cdot e^{-\mu}) \\
\end{align*}

\inputminted[frame=lines, breaklines]{r}{code/q2b.r}

\textbf{c)} Provide a R function enabling to compute the log-posterior $p(\vec{\theta}|\mathcal{D}_i)$ for given $\theta = \theta_0, \theta_1, \theta_2$, data $\mathcal{D}_i$ and large variance priors.

\begin{center}\rule{6cm}{0.4pt}\end{center}

We have no information on the distribution of any parameter $\theta_k$, only that it should have large variance. We will then use a non-informative prior. We could choose for example a Gamma distribution with paramaters $a = 1/2$ and $b = 0.001$ in order to have large variance priors but such a prior distribution received some critiscism in the litterature\footnote{http://www.stat.columbia.edu/\%7Egelman/research/published/taumain.pdf}. An alternative is to use a prior Normal distribution with a large variance of $100$,
\begin{equation}
	\theta_k \sim \mathcal{N}(\mu_0 = 0, \sigma^2_0 = 100)
\end{equation}

Its PDF is given by,
\begin{equation}
	f(\theta ; \mu_0, \sigma_0) = \frac{1}{\sigma_0\sqrt{2\pi}} e^{-\frac{1}{2}\left( \frac{\theta - \mu_0}{\sigma_0} \right)^2}
\end{equation}

We find then,
\begin{align*}
	p(\vec{\theta}|\mathcal{D}_i)
		&\propto \ln(L(\vec{\theta}|\mathcal{D}_i) \cdot (P(\theta_0) P(\theta_1) P(\theta_2))) \\
		&\propto l(\vec{\theta}|\mathcal{D}_i) + \ln\left( P(\theta_0) P(\theta_1) P(\theta_2)) \right) \\
		&\propto \sum_{j=1}^{99} \ln(\mu^{y_{i,j}} \cdot e^{-\mu}) + \ln(f(\theta_0 ; \mu_0, \sigma_0)) + \ln(f(\theta_1 ; \mu_0, \sigma_0)) + \ln(f(\theta_2 ; \mu_0, \sigma_0)) 
\end{align*}

\inputminted[frame=lines, breaklines]{r}{code/q2c.r}

\textbf{d)} Using the preceding R function, obtain the mean vector and variance-covariance matrix in the Laplace approximation to the marginal posterior $\bm{\theta}$ given the data from experiment 1.

\begin{center}\rule{6cm}{0.4pt}\end{center}

\inputminted{r}{code/q2d.r}

We get the following mean vector $\bar{\bm{\theta}}$ and variance-covariance matrix $\Sigma_{\bm{\theta}}$,
\begin{equation}
	\bar{\bm{\theta}} = 
	\begin{pmatrix}
		6.380560  & 1.168446 & -5.224318 
	\end{pmatrix}^T
\end{equation}

\begin{equation}
	\Sigma_{\bm{\theta}} =
	\begin{pmatrix}
		\num{1.002258e-03} & \num{-9.989023e-05} & \num{-0.0011528352} \\
		\num{-9.989023e-05} & \num{2.030509e-04} & \num{0.0002622594} \\
		\num{-1.152835e-03} & \num{2.622594e-04} & \num{0.0014861488}
	\end{pmatrix}
\end{equation}