\subsection*{Question 2}

\textbf{a)} Assuming independence between the data $\mathcal{D}_i$ collected during the experiment $i$ where $\mathcal{D}_i = \set{y_i(t_j): j = 1,\dots,J = 99}$ with $y_i(t_j) \sim \text{Pois}(\mu(t_j))$, provide an analytic form for the likelihood function $L(\vec{\alpha}|\mathcal{D}_i)$ for experiment $i$.

\begin{center}\rule{6cm}{0.4pt}\end{center}

We know that the number of cancer cells $y_{i,j}$ for the experiment $i$ at any given time $t_j$, $j \in \set{10, 15, 20,...,500}$ follow a Poisson distribution of parameter $\mu$. Its probability mass function is given by,
\begin{equation*}
	p(y_{i,j}) \propto \mu^{y_{i,j}} \cdot e^{-\mu} 
\end{equation*}
where an expression for $\mu$ is given above (\autoref{eq:mu_alpha}) as a function of $\alpha_k (k = 0, 1, 2)$.

Assuming independance, we then have,
\begin{align*}
	L(\vec{\alpha}|\mathcal{D}_i) 
		&= L((\alpha_0, \alpha_1, \alpha_2)|y_i) \\
		&= \prod_{j=1}^{99} p(y_{i, j}) \\
		&= \prod_{j=1}^{99} \left( \mu^{y_{i,j}} \cdot e^{-\mu} \right)
\end{align*}

\textbf{b)} Provide a R function enabling to compute the log-likelihood for given parameter values $\theta_k = \log(\alpha_k)$ and data $D_i$.

\begin{center}\rule{6cm}{0.4pt}\end{center}

We have,
\begin{align*}
	l(\vec{\alpha}|\mathcal{D}_i) 
		&:= \ln(L((\alpha_0, \alpha_1, \alpha_2)|y_i)) \\
		&\propto \sum_{j=1}^{99} \ln\left( \left[ \mu^{y_{i,j}} \cdot e^{-\mu} \right] \right) \\
		&= \sum_{j=1}^{99} \ln(\mu^{y_{i,j}} \cdot e^{-\mu}) \\
\end{align*}

\inputminted[frame=lines, breaklines]{r}{code/q2b.r}

\textbf{c)} Provide a R function enabling to compute the log-posterior $p(\vec{\theta}|\mathcal{D}_i)$ for given $\theta = \theta_0, \theta_1, \theta_2$, data $\mathcal{D}_i$ and large variance priors.

\begin{center}\rule{6cm}{0.4pt}\end{center}

We have no information on the distribution of any parameter $\theta_k$, only that it should have large variance. We will then use a non-informative prior. We could choose for example a Gamma distribution with paramaters $a = 1/2$ and $b = 0.001$ in order to have large variance priors but such a prior distribution received some critiscism in the litterature\footnote{http://www.stat.columbia.edu/\%7Egelman/research/published/taumain.pdf}. Therefore, we chose that use a prior uniform distribution,
\begin{equation}
	\theta_k \sim \text{Unif}(0, 100)
\end{equation}

We find then,
\begin{align*}
	p(\vec{\theta}|\mathcal{D}_i)
		&\propto \ln(L(\vec{\theta}|\mathcal{D}_i) \cdot (P(\theta_0) P(\theta_1) P(\theta_2))) \\
		&\propto l(\vec{\theta}|\mathcal{D}_i) + 3 \cdot 1_{0, 100} \\
		&\propto \sum_{j=1}^{99} y_{i, j} \cdot \ln(\mu^{y_{i,j}} \cdot e^{-\mu}) + \ln(f(\theta_0, \lambda)) + 3 \cdot 1_{0, 100} 
\end{align*}

\inputminted[frame=lines, breaklines]{r}{code/q2c.r}

\textbf{d)} Using the preceding R function, obtain the mean vector and variance-covariance matrix in the Laplace approximation to the marginal posterior $\bm{\theta}$ given the data from experiment 1.

\begin{center}\rule{6cm}{0.4pt}\end{center}

\inputminted{r}{code/q2d.r}

We get the following mean vector $\bar{\bm{\theta}}$ and variance-covariance $\Sigma_{\bm{\theta}}$,
\begin{equation}
	\bar{\bm{\theta}} = 
	\begin{pmatrix}
		5.541 &  0.063 & 0.489
	\end{pmatrix}^T
\end{equation}

\begin{equation}
	\Sigma_{\bm{\theta}} =
	\begin{pmatrix}
		\num{3.960386e-05} & \num{-3.595244e-12} & \num{3.484348e-09} \\
		\num{-3.595244e-12} & \num{-4.018152e-03} & \num{-3.398826e-07} \\
		\num{3.484348e-09} & \num{-3.398826e-07} & \num{-2.387783e-01}
	\end{pmatrix}
\end{equation}